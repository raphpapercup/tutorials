{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "gpu_id = 2\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"{}\".format(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import distributions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "X, y = noisy_moons\n",
    "X = StandardScaler().fit_transform(X)\n",
    "xlim, ylim = [-2, 2], [-2, 2]\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10, color='red')\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_n01(N):\n",
    "    # Sample from a normal(0, 1) distribution.\n",
    "    D = 2\n",
    "    return np.random.normal(size = (N, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample points from random normal(0, I_2). \n",
    "# Shift the distribution to (1, 1). \n",
    "# Keep track of both distributions.\n",
    "X_normal_shifted = sample_n01(1000) + 1\n",
    "X_normal = sample_n01(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], s=10, color='red', alpha=1)\n",
    "plt.scatter(X_normal[:, 0], X_normal[:, 1], s=10, color='green', alpha=0.5)\n",
    "# plt.scatter(X_normal_shifted[:, 0], X_normal_shifted[:, 1], s=10, color='blue', alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_n01(x):\n",
    "    # Evaluate log likelihood under the normal distribution.\n",
    "    return np.sum(- np.square(x) / 2 - np.log(np.sqrt(2 * np.pi)), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loglikelihood of the two moons data under the normal distribution.\n",
    "Under the two moons data, this data is clearly bimodal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(log_prob_n01(X), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For comparison, loglikelihood of the normal(0, I_2) distributed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(log_prob_n01(X_normal), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NVP(nn.Module):\n",
    "    def __init__(self, flips, D=2):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.flips = flips\n",
    "        self.prior = distributions.MultivariateNormal(torch.zeros(2), torch.eye(2))\n",
    "        self.shift_log_scale_fns = nn.ModuleList()\n",
    "        for _ in flips:\n",
    "            shift_log_scale_fn = nn.Sequential(\n",
    "                nn.Linear(1, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, D),\n",
    "            )\n",
    "            self.shift_log_scale_fns.append(shift_log_scale_fn)\n",
    "    \n",
    "    def forward(self, x, flip_idx):\n",
    "        # x is of shape [B, H]\n",
    "        flip = self.flips[flip_idx]\n",
    "        d = x.shape[-1] // 2\n",
    "        x1, x2 = x[:, :d], x[:, d:]\n",
    "        if flip:\n",
    "            x2, x1 = x1, x2\n",
    "        net_out = self.shift_log_scale_fns[flip_idx](x1)\n",
    "        shift = net_out[:, :self.D // 2]\n",
    "        log_scale = net_out[:, self.D // 2:]\n",
    "        y2 = x2 * torch.exp(log_scale) + shift\n",
    "        if flip:\n",
    "            x1, y2 = y2, x1\n",
    "        y = torch.cat([x1, y2], -1)\n",
    "        return y\n",
    "    \n",
    "    def inverse_forward(self, y, flip_idx):\n",
    "        flip = self.flips[flip_idx]\n",
    "        d = y.shape[-1] // 2\n",
    "        y1, y2 = y[:, :d], y[:, d:]\n",
    "        if flip:\n",
    "            y1, y2 = y2, y1\n",
    "        net_out = self.shift_log_scale_fns[flip_idx](y1)\n",
    "        shift = net_out[:, :self.D // 2]\n",
    "        log_scale = net_out[:, self.D // 2:]\n",
    "        x2 = (y2 - shift) * torch.exp(-log_scale)\n",
    "        if flip:\n",
    "            y1, x2 = x2, y1\n",
    "        x = torch.cat([y1, x2], -1)\n",
    "        return x, log_scale\n",
    "    \n",
    "    @staticmethod\n",
    "    def base_log_prob_fn(x):\n",
    "        return torch.sum(- (x ** 2) / 2 - np.log(np.sqrt(2 * np.pi)), -1)\n",
    "    \n",
    "    def base_sample_fn(self, N):\n",
    "        # sampler random normal(0, I)\n",
    "        x = self.prior.sample((N, 1)).cuda().squeeze(1)\n",
    "        return x\n",
    "        \n",
    "    def log_prob(self, y, flip_idx):\n",
    "        x, log_scale = self.inverse_forward(y, flip_idx)\n",
    "        # This comes from the jacobian. In this case the jacobian is simply the product of the scales,\n",
    "        # which becomes the sum of log scales in the loglikelihood.\n",
    "        ildj = - torch.sum(log_scale, -1)\n",
    "        return self.base_log_prob_fn(x) + ildj\n",
    "    \n",
    "    def sample_nvp_chain(self, N):\n",
    "        xs = []\n",
    "        x = self.base_sample_fn(N)\n",
    "        xs.append(x)\n",
    "        for i, _ in enumerate(self.flips):\n",
    "            x = self.forward(x, flip_idx=i)\n",
    "            xs.append(x)\n",
    "        return x, xs\n",
    "    \n",
    "    def log_prob_chain(self, y):\n",
    "        # Run y through all the necessary inverses, keeping track\n",
    "        # of the logscale along the way, allowing us to compute the loss.\n",
    "        temp = y\n",
    "        logscales = y.data.new(y.shape[0]).zero_()\n",
    "        for i, _ in enumerate(self.flips):\n",
    "            temp, logscale = self.inverse_forward(\n",
    "                temp, \n",
    "                flip_idx=len(self.flips) - 1 - i,\n",
    "            )\n",
    "            # One logscale per element in a batch per layer of flow.\n",
    "            logscales += logscale.squeeze(-1)\n",
    "        return self.base_log_prob_fn(temp) - logscales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flips = [False, True, False, True, False, True]\n",
    "my_nvp = NVP(flips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loglikelihood of the two moons data under this new distribution.\n",
    "These are still quite low and bimodal: makes sense, the model hasnt been trained yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(my_nvp.log_prob_chain(torch.FloatTensor(X)).data.cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Here we decide to stack six layers.\"\"\"\n",
    "flips = [False, True, False, True, False, True]\n",
    "learning_rate = 1e-4\n",
    "model = NVP(flips).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_enum = range(iters - 1)\n",
    "min_loss = float('inf')\n",
    "for i in train_enum:\n",
    "    noisy_moons = datasets.make_moons(n_samples=128, noise=.05)[0].astype(np.float32)\n",
    "    optimizer.zero_grad()\n",
    "    batch = torch.FloatTensor(noisy_moons).cuda()\n",
    "    loss = - torch.mean(model.log_prob_chain(batch))\n",
    "    if loss.item() < min_loss:\n",
    "        bestmodel = model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 500 == 0:\n",
    "        print('Iter {}, loss is {:.3f}'.format(i, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_Xs, _ = bestmodel.sample_nvp_chain(10000)\n",
    "new_Xs = new_Xs.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(new_Xs[:, 0], new_Xs[:, 1], c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make that GIF now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML, Image\n",
    "\n",
    "def animate(i):\n",
    "    l = i//48\n",
    "    t = (float(i%48))/48\n",
    "    y = (1-t)*xs_list[l] + t*xs_list[l+1]\n",
    "    paths.set_offsets(y)\n",
    "    return (paths,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Xs, xs_list = bestmodel.sample_nvp_chain(10000)\n",
    "new_Xs = new_Xs.data.cpu().numpy()\n",
    "xs_list = [x.data.cpu().numpy() for x in xs_list]\n",
    "plt.show()\n",
    "\n",
    "for x in xs_list:\n",
    "    plt.scatter(x[:, 0], x[:, 1], c='r', s=1)\n",
    "    plt.show()\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "paths = ax.scatter(xs_list[0][:, 0], xs_list[0][:, 1], s=1, color='red')\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, frames=48*len(flips), interval=1, blit=False)\n",
    "anim.save('new_anim_circle_curve.gif', writer='imagemagick', fps=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
