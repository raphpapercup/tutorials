{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "gpu_id = 0\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"{}\".format(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import distributions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get two moons data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "noise = .1\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=noise)\n",
    "X, y = noisy_moons\n",
    "X = StandardScaler().fit_transform(X)\n",
    "xlim, ylim = [-3, 3], [-3, 3]\n",
    "colors = ['red' if label == 0 else 'blue' for label in y]\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10, color=colors)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classifier network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class twomoons_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"x is [B, 2]\"\"\"\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick question? What loss are we going to train with\n",
    "Well, expected risk minimisation is cross entropy for classification and min square error for regression. Turns out they are both equivalent to maximum likelihood estimation. So we train this classification task with MSE loss, so that afterwards we'll be able to train the mixup network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train first with noise = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "noise = .02\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=noise)\n",
    "X, y = noisy_moons\n",
    "X = StandardScaler().fit_transform(X)\n",
    "xlim, ylim = [-3, 3], [-3, 3]\n",
    "colors = ['red' if label == 0 else 'blue' for label in y]\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10, color=colors)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Setup model\"\"\"\n",
    "model = twomoons_classifier().cuda()\n",
    "num_iters = 10000\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\"\"\"Train model\"\"\"\n",
    "train_loss = []\n",
    "for iter in range(num_iters):\n",
    "    X, y = datasets.make_moons(n_samples=128, noise=noise)\n",
    "    X = X.astype(np.float32)\n",
    "    logits = model(torch.FloatTensor(X).cuda())\n",
    "    loss = F.mse_loss(F.sigmoid(logits).squeeze(1), torch.FloatTensor(y).cuda())\n",
    "    train_loss.append(loss.item())\n",
    "    if iter % 1000 == 0:\n",
    "        print('Iter {} Loss: {:.3f}'.format(iter, loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\"\"\"Plot losses\"\"\"\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now train with noise = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "noise = .3\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=noise)\n",
    "X, y = noisy_moons\n",
    "X = StandardScaler().fit_transform(X)\n",
    "xlim, ylim = [-3, 3], [-3, 3]\n",
    "colors = ['red' if label == 0 else 'blue' for label in y]\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10, color=colors)\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setup model\"\"\"\n",
    "model = twomoons_classifier().cuda()\n",
    "num_iters = 10000\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\"\"\"Train model\"\"\"\n",
    "train_loss_no_mixup = []\n",
    "for iter in range(num_iters):\n",
    "    X, y = datasets.make_moons(n_samples=128, noise=noise)\n",
    "    X = X.astype(np.float32)\n",
    "    logits = model(torch.FloatTensor(X).cuda())\n",
    "    loss = F.mse_loss(F.sigmoid(logits).squeeze(1), torch.FloatTensor(y).cuda())\n",
    "    train_loss_no_mixup.append(loss.item())\n",
    "    if iter % 1000 == 0:\n",
    "        print('Iter {} Loss: {:.3f}'.format(iter, loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "\"\"\"Plot losses\"\"\"\n",
    "plt.plot(train_loss_no_mixup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now train with noise = 0.2 and mixup. Note, this is now MSE instead of cross entropy!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setup model\"\"\"\n",
    "model_mixup = twomoons_classifier().cuda()\n",
    "num_iters = 10000\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model_mixup.parameters(), lr=learning_rate)\n",
    "# specific to mixup.\n",
    "alpha = 0.4\n",
    "\n",
    "\"\"\"Train\"\"\"\n",
    "train_loss_mixup = []\n",
    "for iter in range(num_iters):\n",
    "    \"\"\"Draw twice.\"\"\"\n",
    "    X1, y1 = datasets.make_moons(n_samples=128, noise=noise)\n",
    "    X2, y2 = datasets.make_moons(n_samples=128, noise=noise)\n",
    "    X1 = X1.astype(np.float32)\n",
    "    X2 = X2.astype(np.float32)\n",
    "    \"\"\"Draw from beta distribution\"\"\"\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    X = lam * X1 + (1 - lam) * X2\n",
    "    y = lam * y1 + (1 - lam) * y2\n",
    "\n",
    "    logits = model_mixup(torch.FloatTensor(X).cuda())\n",
    "    loss = F.mse_loss(F.sigmoid(logits).squeeze(1), torch.FloatTensor(y).cuda())\n",
    "    train_loss_mixup.append(loss.item())\n",
    "    if iter % 1000 == 0:\n",
    "        print('Iter {} Loss: {:.3f}'.format(iter, loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare two training losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_loss_mixup, alpha=0.5, color='red', label='With mixup')\n",
    "plt.plot(train_loss_no_mixup, alpha=0.5, color='blue', label='No mixup')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
